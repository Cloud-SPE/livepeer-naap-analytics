services:
  ### webserver ###
  webserver:
    image: tztcloud/livepeer-rtav-test-ui:latest
    container_name: live-video-to-video-webserver
    ports:
      - 8088:8088

  ###  INGEST  ###
  mediamtx:
    image: livepeerci/mediamtx
    container_name: live-video-to-video-mediamtx
    volumes:
      - ./configs/mediamtx/mediamtx.yml:/mediamtx.yml
    environment:
      - MTX_WEBRTCADDITIONALHOSTS=${HOST}
    ports:
      - 8890:8890
      - 9997:9997
      - 1935:1935

  gateway:
    image: livepeer/go-livepeer:latest
    container_name: live-video-to-video-gateway
    volumes:
      - ./data/gateway:/data
    ports:
      - 5937:5937
      - 7280:7280
    environment:
      - LIVE_AI_ALLOW_CORS=1
      - LIVE_AI_WHIP_ADDR=gateway:7280
#       input LIVE_AI_GATHER_TIMEOUT as integer (parses to seconds)
      - LIVE_AI_GATHER_TIMEOUT=5
#      input LIVE_AI_MIN_SEG_DUR as duration string (e.g. 1s)
      - LIVE_AI_MIN_SEG_DUR=1s
      - LIVE_AI_NAT_IP=${HOST_IP}
      - LIVE_AI_PLAYBACK_HOST=rtmp://mediamtx:1935/
#      should match to the mediamtx webrtcAddress in mediamtx.yml
      - LIVE_AI_WHEP_URL=https://${HOST}/mediamtx/
    command: ["-gateway",
          "-orchAddr=${ORCH_SERVICE_ADDR}",
          "-rtmpAddr=gateway:1937",
          "-httpAddr=gateway:5937",
          "-httpIngest=true",
          "-v=9",
          "-network=arbitrum-one-mainnet",
          "-blockPollingInterval=10",
          "-ethUrl=${ARB_ETH_URL}",
          "-ethPassword=/data/eth-secret.txt",
          "-ethKeystorePath=/data/",
          "-dataDir=/data/tmp",
          "-monitor",
              "-kafkaBootstrapServers=kafka:9092",
              "-kafkaGatewayTopic=streaming-events" ]

  ####  Monitoring  ####
  kafka:
    image: apache/kafka:3.9.0
    container_name: live-video-to-video-kafka
    ports:
      - 9092:9092
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'

      # FIX 1: Explicitly use 127.0.0.1 for the quorum
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@127.0.0.1:9093'

      # FIX 2: List both your data port (9092) and controller port (9093)
      KAFKA_LISTENERS: 'PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093'

      # FIX 3: THE MOST IMPORTANT PART FOR 3.9.0
      # You MUST explicitly list the CONTROLLER in advertised listeners to
      # stop Kafka from defaulting it to 0.0.0.0 and crashing.
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:9092,CONTROLLER://127.0.0.1:9093'

      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

      # Use the correct variable name for official Apache image (no CFG_ prefix)
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      # Increase log retention for replay capability
      KAFKA_LOG_RETENTION_HOURS: 168  # 7 days
      KAFKA_LOG_SEGMENT_BYTES: 1073741824  # 1GB segments
      KAFKA_LOG_DIRS: '/var/lib/kafka/data'
    volumes:
      - ./data/kafka:/var/lib/kafka/data
    healthcheck:
      test: /opt/kafka/bin/kafka-cluster.sh cluster-id --bootstrap-server kafka:9092 || exit 1
      interval: 1s
      timeout: 60s
      retries: 60

  kafka-sse-api:
    image: ghcr.io/aklivity/zilla:latest
    container_name: live-video-to-video-kafka-sse-api
    ports:
      - 7114:7114
    environment:
      KAFKA_BOOTSTRAP_SERVER: kafka:9092
      ZILLA_INCUBATOR_ENABLED: "true"
    volumes:
      - ./configs/zilla:/etc/zilla
      - ./configs/zilla/index.html:/var/www/index.html
    command: start -v -e
    depends_on:
      kafka:
        condition: service_healthy
        restart: true

  kafka-ui:
    image: ghcr.io/kafbat/kafka-ui:latest
    container_name: live-video-to-video-kafka-ui
    ports:
      - 8080:8080
    environment:
      KAFKA_CLUSTERS_0_NAME: live-video-to-video
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    depends_on:
      kafka:
        condition: service_healthy
        restart: true

  kafka-init:
    image: apache/kafka:3.9.0
    container_name: live-video-to-video-kafka-init
    command:
      - /bin/sh
      - -c
      - |
        /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic streaming-events --partitions 3 --replication-factor 1
#        /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic streaming-events-cleansed --partitions 3 --replication-factor 1
#        /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic calculated-metrics --partitions 3 --replication-factor 1
    depends_on:
      kafka:
        condition: service_healthy
        restart: true
    init: true

  ### STREAM PROCESSING ###
  # Build the Flink job JAR
  flink-builder:
    image: sbtscala/scala-sbt:eclipse-temurin-jammy-11.0.20.1_1_1.9.7_2.12.18
    container_name: livepeer-analytics-flink-builder
    working_dir: /build
    volumes:
      - ./flink-jobs:/build
      - ./data/flink/jars:/build/target/scala-2.12  # Output JAR here
      - flink-ivy-cache:/root/.ivy2
      - flink-sbt-cache:/root/.sbt
    command: >
      sh -c "
        echo 'Building Flink jobs...';
        sbt clean assembly;
        echo 'Build complete. JAR available at target/scala-2.12/';
        ls -lh target/scala-2.12/*.jar;
      "
    init: true

  flink-jobmanager:
    image: flink:1.18-java11
    container_name: livepeer-analytics-flink-jobmanager
    ports:
      - "8081:8081"  # Flink Web UI
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        parallelism.default: 2
        state.backend: rocksdb
        state.checkpoints.dir: file:////flink-tmp/checkpoints
        state.savepoints.dir: file:////flink-tmp/savepoints
        state.backend.incremental: true
        execution.checkpointing.interval: 60000
        execution.checkpointing.min-pause: 30000
        execution.checkpointing.timeout: 600000
    volumes:
      - ./data/flink/tmp:/flink-tmp
      - ./data/flink/jars:/opt/flink/usrlib  # Shared volume with builder
    depends_on:
      flink-builder:
        condition: service_completed_successfully
      kafka:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/overview"]
      interval: 10s
      timeout: 5s
      retries: 5

  flink-taskmanager:
    image: flink:1.18-java11
    container_name: livepeer-analytics-flink-taskmanager
    depends_on:
      - flink-jobmanager
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
        parallelism.default: 2
        state.backend: rocksdb
        state.backend.incremental: true
    volumes:
      - ./data/flink/tmp/checkpoints:/tmp/flink-checkpoints
      - ./data/flink/tmp/savepoints:/tmp/flink-savepoints
      - ./data/flink/jars:/opt/flink/usrlib
    scale: 1

  flink-job-submitter:
    image: flink:1.18-java11
    container_name: livepeer-analytics-flink-job-submitter
    depends_on:
      flink-jobmanager:
        condition: service_started
      flink-taskmanager:
        condition: service_started
    environment:
      -| FLINK_PROPERTIES=jobmanager.rpc.address: flink-jobmanager
    volumes:
      - ./data/flink/jars:/opt/flink/usrlib
      - ./flink-jobs/:/jobs/
    command: >
      sh -c "
        echo 'Waiting for Flink cluster to be ready...';
        sleep 30;
      
        echo 'Submitting Flink jobs...';
        /bin/bash /jobs/submit-jobs.sh;
      
        echo 'Job submission complete.';
      "
    init: true

  ### ANALYTICS STORAGE ###
  clickhouse:
    image: clickhouse/clickhouse-server:24.11
    container_name: livepeer-analytics-clickhouse
    ports:
      - "8123:8123" # HTTP Interface
      #- "9000:9000" # Native Interface
    volumes:
      - ./data/clickhouse:/var/lib/clickhouse
      - ./configs/clickhouse-users:/etc/clickhouse-server/users.d
      - ./configs/clickhouse-init:/docker-entrypoint-initdb.d  # Schema initialization
    environment:
      CLICKHOUSE_DB: livepeer_analytics
      CLICKHOUSE_USER: analytics_user
      CLICKHOUSE_PASSWORD: analytics_password
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "localhost:8123/ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  minio:
    image: minio/minio:latest
    container_name: live-video-to-video-minio
    ports:
      #- "9000:9000" # API
      - "9001:9001" # Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - ./data/minio:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 5s
      retries: 5

  minio-init:
    image: minio/mc:latest
    container_name: live-video-to-video-minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 minioadmin minioadmin;
      mc mb myminio/livepeer-events --ignore-existing;
      mc mb myminio/livepeer-metrics --ignore-existing;
      mc anonymous set download myminio/livepeer-events;
      echo 'MinIO buckets created';
      "
    init: true

  ### VISUALIZATION ###
  grafana:
    image: grafana/grafana:latest
    container_name: livepeer-analytics-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=grafana-clickhouse-datasource
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/etc/grafana/provisioning/dashboards/naap-overview.json
    volumes:
      - ./data/grafana:/var/lib/grafana
      - ./configs/grafana/provisioning:/etc/grafana/provisioning
#      - ./configs/grafana/dashboards:/etc/grafana/provisioning/dashboards
    depends_on:
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

# Add volumes for caching dependencies (speeds up builds)
volumes:
  flink-ivy-cache:
  flink-sbt-cache:

networks:
  default:
    name: live-video-to-video