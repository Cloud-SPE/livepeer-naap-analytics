#!/usr/bin/env python3
"""Replay scenario fixture raw events to Kafka for end-to-end Flink validation.

This script reads fixture JSONL files from a manifest generated by
`scripts/export_scenario_fixtures.py` and publishes raw event envelopes to the
input topic consumed by Flink (`streaming_events` by default).

Supported source records inside fixture JSONL:
- ai_stream_status (uses raw_json)
- stream_trace_events (uses raw_json)
- ai_stream_events (uses raw_json)
- stream_ingest_metrics (uses raw_json)
- network_capabilities (uses raw_json)
- streaming_events (reconstructs raw envelope directly from columns)
"""

from __future__ import annotations

import argparse
import json
import subprocess
import sys
import time
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

RAW_EVENT_TABLES = {
    "ai_stream_status",
    "stream_trace_events",
    "ai_stream_events",
    "stream_ingest_metrics",
    "network_capabilities",
    "streaming_events",
}


@dataclass
class ReplayEvent:
    payload: str
    ts_ms: int
    scenario: str
    source_table: str
    source_file: str


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Replay scenario fixture events to Kafka topic")
    parser.add_argument("--manifest", required=True, help="Fixture manifest.json")
    parser.add_argument(
        "--scenario",
        action="append",
        help="Optional scenario name filter; repeat flag for multiple scenarios",
    )
    parser.add_argument("--kafka-container", default="live-video-to-video-kafka")
    parser.add_argument("--bootstrap-server", default="kafka:9092")
    parser.add_argument("--topic", default="streaming_events")
    parser.add_argument("--sleep-ms", type=int, default=0, help="Pause between messages")
    parser.add_argument("--max-events", type=int, default=0, help="Optional cap for replayed events")
    parser.add_argument("--dry-run", action="store_true", help="Parse fixtures and report counts only")
    parser.add_argument("--json-out", help="Optional JSON output path with replay summary")
    return parser.parse_args()


def load_manifest(path: Path) -> dict[str, Any]:
    return json.loads(path.read_text(encoding="utf-8"))


def resolve_fixture_files(
    manifest: dict[str, Any], manifest_path: Path, scenario_filter: set[str]
) -> tuple[list[tuple[str, Path]], dict[str, Any]]:
    pairs: list[tuple[str, Path]] = []
    # Defensive dedupe: if a manifest accidentally references the same file more
    # than once for a scenario, replay should still be idempotent.
    seen_pairs: set[tuple[str, str]] = set()
    pair_counts: dict[tuple[str, str], int] = {}
    base = manifest_path.parent
    repo_root = Path(__file__).resolve().parent.parent
    scenarios = manifest.get("scenarios", {})
    for scenario_name, scenario in scenarios.items():
        if scenario_filter and scenario_name not in scenario_filter:
            continue
        for session in scenario.get("sessions", []):
            p = Path(session.get("file", ""))
            if not p:
                continue
            if not p.is_absolute():
                base_relative = (base / p).resolve()
                repo_relative = (repo_root / p).resolve()
                if base_relative.exists():
                    p = base_relative
                elif repo_relative.exists():
                    p = repo_relative
                elif p.parts[:3] == ("tests", "integration", "fixtures"):
                    p = repo_relative
                else:
                    p = base_relative
            key = (scenario_name, str(p))
            pair_counts[key] = pair_counts.get(key, 0) + 1
            if key in seen_pairs:
                continue
            seen_pairs.add(key)
            pairs.append((scenario_name, p))
    duplicate_details = [
        {
            "scenario": scenario,
            "file": file_path,
            "occurrences": count,
            "dropped": count - 1,
        }
        for (scenario, file_path), count in sorted(pair_counts.items())
        if count > 1
    ]
    fixture_resolution = {
        "files_referenced_before_dedupe": sum(pair_counts.values()),
        "files_replayed_after_dedupe": len(pairs),
        "duplicate_references_dropped": sum(int(item["dropped"]) for item in duplicate_details),
        "duplicate_references": duplicate_details,
    }
    return pairs, fixture_resolution


def parse_ts_ms(obj: dict[str, Any]) -> int:
    value = obj.get("timestamp")
    if value is None:
        return 0
    try:
        return int(value)
    except (TypeError, ValueError):
        return 0


def parse_record_iso_ts_ms(value: Any) -> int:
    if not isinstance(value, str) or not value.strip():
        return 0
    try:
        dt = datetime.fromisoformat(value.replace("Z", "+00:00"))
    except ValueError:
        return 0
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return int(dt.timestamp() * 1000)


def extract_raw_event(record: dict[str, Any]) -> dict[str, Any] | None:
    table = record.get("__table")
    if table not in RAW_EVENT_TABLES:
        return None

    if table == "network_capabilities":
        # network_capabilities fixtures are stored as typed rows; rebuild the
        # raw envelope expected on Kafka input from typed columns + raw_json.
        raw_json = record.get("raw_json")
        if not isinstance(raw_json, str) or not raw_json.strip():
            return None
        try:
            data_obj = json.loads(raw_json)
        except json.JSONDecodeError:
            return None
        if not isinstance(data_obj, dict):
            return None
        event_id = str(record.get("source_event_id", "")).strip()
        if not event_id:
            return None
        ts_ms = parse_record_iso_ts_ms(record.get("event_timestamp"))
        if ts_ms <= 0:
            return None
        return {
            "id": event_id,
            "type": "network_capabilities",
            "timestamp": str(ts_ms),
            "gateway": "",
            "data": data_obj,
        }

    if table == "streaming_events":
        # streaming_events rows are already raw-like; preserve their original
        # envelope shape, normalizing `data` when it is serialized as a string.
        required = ["id", "type", "timestamp", "gateway", "data"]
        if any(k not in record for k in required):
            return None
        data_value = record.get("data")
        if isinstance(data_value, str):
            try:
                # Some rows store a JSON string; preserve structured object when possible.
                data_value = json.loads(data_value)
            except json.JSONDecodeError:
                pass
        return {
            "id": record.get("id"),
            "type": record.get("type"),
            "timestamp": str(record.get("timestamp")),
            "gateway": record.get("gateway", ""),
            "data": data_value,
        }

    raw_json = record.get("raw_json")
    if not isinstance(raw_json, str) or not raw_json.strip():
        return None
    try:
        payload = json.loads(raw_json)
    except json.JSONDecodeError:
        return None
    if not isinstance(payload, dict):
        return None
    required = ["id", "type", "timestamp", "gateway", "data"]
    if any(k not in payload for k in required):
        return None
    return payload


def collect_events(files: list[tuple[str, Path]], max_events: int) -> tuple[list[ReplayEvent], dict[str, Any]]:
    events: list[ReplayEvent] = []
    stats = {
        "files_total": len(files),
        "files_missing": 0,
        "rows_seen": 0,
        "rows_eligible": 0,
        "rows_replayable": 0,
        "rows_skipped": 0,
        "by_table": defaultdict(int),
        "by_scenario": defaultdict(int),
    }

    for scenario_name, file_path in files:
        if not file_path.exists():
            stats["files_missing"] += 1
            continue

        with file_path.open("r", encoding="utf-8") as fh:
            for line in fh:
                line = line.strip()
                if not line:
                    continue
                stats["rows_seen"] += 1
                try:
                    record = json.loads(line)
                except json.JSONDecodeError:
                    stats["rows_skipped"] += 1
                    continue

                table = record.get("__table")
                if table not in RAW_EVENT_TABLES:
                    continue
                stats["rows_eligible"] += 1

                payload_obj = extract_raw_event(record)
                if payload_obj is None:
                    stats["rows_skipped"] += 1
                    continue

                payload = json.dumps(payload_obj, ensure_ascii=False, separators=(",", ":"))
                ts_ms = parse_ts_ms(payload_obj)
                events.append(
                    ReplayEvent(
                        payload=payload,
                        ts_ms=ts_ms,
                        scenario=scenario_name,
                        source_table=str(table),
                        source_file=str(file_path),
                    )
                )
                stats["rows_replayable"] += 1
                stats["by_table"][str(table)] += 1
                stats["by_scenario"][scenario_name] += 1

                if max_events > 0 and len(events) >= max_events:
                    break

        if max_events > 0 and len(events) >= max_events:
            break

    # Replay in deterministic timestamp order to reduce event-order drift
    # across local reruns and CI.
    events.sort(key=lambda e: (e.ts_ms, e.scenario, e.source_table))

    stats["by_table"] = dict(sorted(stats["by_table"].items()))
    stats["by_scenario"] = dict(sorted(stats["by_scenario"].items()))
    stats["events_collected"] = len(events)
    return events, stats


def replay_events(args: argparse.Namespace, events: list[ReplayEvent]) -> None:
    if not events:
        raise RuntimeError("No replayable events found in manifest fixtures")

    cmd = [
        "docker",
        "exec",
        "-i",
        args.kafka_container,
        "/opt/kafka/bin/kafka-console-producer.sh",
        "--bootstrap-server",
        args.bootstrap_server,
        "--topic",
        args.topic,
    ]

    # Fast path: single producer process for all events.
    payload = "\n".join(e.payload for e in events) + "\n"
    proc = subprocess.run(cmd, input=payload, text=True, capture_output=True)
    if proc.returncode != 0:
        raise RuntimeError(
            f"Kafka replay command failed rc={proc.returncode}: {proc.stderr.strip() or proc.stdout.strip()}"
        )

    if args.sleep_ms > 0:
        time.sleep((args.sleep_ms * len(events)) / 1000.0)


def main() -> None:
    args = parse_args()

    manifest_path = Path(args.manifest).resolve()
    if not manifest_path.exists():
        print(f"Manifest not found: {manifest_path}", file=sys.stderr)
        sys.exit(2)

    scenario_filter = set(args.scenario or [])
    manifest = load_manifest(manifest_path)
    files, fixture_resolution = resolve_fixture_files(manifest, manifest_path, scenario_filter)

    if not files:
        print("No fixture files found for selected scenarios", file=sys.stderr)
        sys.exit(2)

    events, stats = collect_events(files, args.max_events)

    if not args.dry_run:
        replay_events(args, events)

    out = {
        "manifest": str(manifest_path),
        "topic": args.topic,
        "kafka_container": args.kafka_container,
        "bootstrap_server": args.bootstrap_server,
        "dry_run": bool(args.dry_run),
        "fixture_resolution": fixture_resolution,
        "stats": stats,
    }

    print(json.dumps(out, indent=2))

    if args.json_out:
        Path(args.json_out).write_text(json.dumps(out, indent=2), encoding="utf-8")


if __name__ == "__main__":
    main()
